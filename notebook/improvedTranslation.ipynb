{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LC1332/Luotuo-Chinese-LLM/blob/main/notebook/improvedTranslation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fB4TiOyU2aPk"
      },
      "source": [
        "# 一个升级后的批量翻译代码\n",
        "\n",
        "这个代码最初由黄泓森进行开发，由李鲁鲁转到colab并进行了更改\n",
        "\n",
        "[骆驼项目主页](https://github.com/LC1332/Luotuo-Chinese-LLM)\n",
        "\n",
        "如果你使用我们的代码获取了有用的数据，也欢迎分享给我们，或者告诉我们你公开后的github/huggingface链接\n",
        "\n",
        "如果你使用我们的代码获取数据并发表了论文或者tech report，欢迎cite我们的github repo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzEAdry5GRz4"
      },
      "source": [
        "## 安装环境"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FY9B-984F_mQ",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "!pip install openai\n",
        "!pip install aiofiles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "A7Ddf_CbGBCE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "import openai\n",
        "import asyncio\n",
        "import aiohttp\n",
        "import aiofiles\n",
        "from functools import partial\n",
        "from tqdm.asyncio import tqdm as tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0rXCoAWGT0d"
      },
      "source": [
        "## 输入你的openAI API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "IWfEZjVbGnon"
      },
      "outputs": [],
      "source": [
        "# 在这里输入你的openAI API token\n",
        "\n",
        "api_key = [\"sk-r4\"]\n",
        "\n",
        "\n",
        "class KeyPool:\n",
        "    def __init__(self, strings):\n",
        "        self.pool = list(strings)\n",
        "        self.last_used = {s: -1 for s in strings}\n",
        "\n",
        "    def getKey(self):\n",
        "        result = min(self.last_used, key=self.last_used.get)\n",
        "        self.last_used[result] = int(time.time() * 1000)\n",
        "        return result\n",
        "\n",
        "pool = KeyPool(api_key)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGsk442ZHJcs"
      },
      "source": [
        "## 指定工作目录\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "e1A5kyTFHpjC"
      },
      "outputs": [],
      "source": [
        "os.chdir(\"/content/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mo43M636J3KT"
      },
      "source": [
        "## 获取需要翻译的样本\n",
        "\n",
        "这里我们使用WizardLM的样本"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "WTYu1i1_Jyxw",
        "outputId": "5fc51a77-80fb-4e5b-ab68-3bf4b0ddada5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-10 03:19:13--  https://raw.githubusercontent.com/LC1332/WizardLM/main/data/WizardLM_testset.jsonl\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 81139 (79K) [text/plain]\n",
            "Saving to: ‘WizardLM_testset.jsonl’\n",
            "\n",
            "\rWizardLM_testset.js   0%[                    ]       0  --.-KB/s               \rWizardLM_testset.js 100%[===================>]  79.24K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2023-05-10 03:19:13 (6.38 MB/s) - ‘WizardLM_testset.jsonl’ saved [81139/81139]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/LC1332/WizardLM/main/data/WizardLM_testset.jsonl -O WizardLM_testset.jsonl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "39UbhftwKDGH",
        "outputId": "8b8e3a27-c153-4ee6-b963-b1dce2cb225d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "delay = 0.05\n",
        "\n",
        "concurrency_limit = 64\n",
        "\n",
        "input_file = \"WizardLM_testset.jsonl\"\n",
        "\n",
        "# 数据缓存目录\n",
        "temp_path = \"/content/temp\"\n",
        "\n",
        "# 数据输出目录\n",
        "output_path = \"/content/translate\"\n",
        "\n",
        "output_prefix = \"WizardLM_tr\"\n",
        "\n",
        "max_file_size = 1024**3\n",
        "\n",
        "# 需要翻译的字段\n",
        "entries = [\"Instruction\"]\n",
        "\n",
        "os.system(f\"mkdir -p {temp_path} {output_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "C7a7AdXAF_mR",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "async def getTranslation(item, entries: list = []):\n",
        "    async def get(text):\n",
        "        text = text.replace(\"\\n\", \" \")\n",
        "        openai.api_key = pool.getKey()\n",
        "        try:\n",
        "            resp = await openai.ChatCompletion.acreate(\n",
        "                model=\"gpt-3.5-turbo\",\n",
        "                messages=[\n",
        "                    {\n",
        "                        \"role\": \"system\",\n",
        "                        \"content\": \"这是一个能够将文本翻译成中文的AI助手。请将引号中的文本翻译成简体中文。\",\n",
        "                    },\n",
        "                    {\"role\": \"user\", \"content\": f'\"\"\"\\n{text}\\n\"\"\"'},\n",
        "                ],\n",
        "                temperature=0.3,\n",
        "                max_tokens=100,\n",
        "                top_p=1.0,\n",
        "                frequency_penalty=0.0,\n",
        "                presence_penalty=0.0,\n",
        "            )\n",
        "            if \"choices\" in resp:\n",
        "                return resp['choices'][0]['message']['content']\n",
        "            else:\n",
        "                raise Exception(f\"Invalid API response: {resp}\")\n",
        "        except Exception as e:\n",
        "            print(f\"[Error] {e}\")\n",
        "            return None\n",
        "\n",
        "    for entry in entries:\n",
        "        trans = await get(item[entry])\n",
        "        if trans is None:\n",
        "            return None\n",
        "        else:\n",
        "            item[f\"{entry}_zh\"] = trans\n",
        "    return item\n",
        "\n",
        "\n",
        "async def process(id, item, semaphore):\n",
        "    async with semaphore:\n",
        "        file_name = f\"{temp_path}/{output_prefix}_{id}.json\"\n",
        "        try:\n",
        "            it = await getTranslation(item, entries)\n",
        "            if it is None:\n",
        "                raise Exception(file_name)\n",
        "            async with aiofiles.open(file_name, \"w\") as f:\n",
        "                await f.write(json.dumps(it, ensure_ascii=False, indent=4))\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving item: {e}\")\n",
        "\n",
        "\n",
        "async def main():\n",
        "    try:\n",
        "      with open(input_file, \"r\") as file:\n",
        "          data = json.load(file)\n",
        "    except json.JSONDecodeError:\n",
        "      data = []\n",
        "      with open(input_file, \"r\") as file:\n",
        "          for line in file:\n",
        "              entry = json.loads(line)\n",
        "              data.append(entry)\n",
        "\n",
        "    tasks = []\n",
        "\n",
        "    semaphore = asyncio.Semaphore(concurrency_limit)\n",
        "\n",
        "    for id, item in enumerate(data):\n",
        "        if os.path.exists(f\"{output_prefix}{id}.json\"):\n",
        "            continue\n",
        "        tasks.append(asyncio.create_task(process(id, item, semaphore)))\n",
        "\n",
        "    async for task in tqdm(tasks, total=len(tasks), desc=\"Processing items\"):\n",
        "        await task\n",
        "        time.sleep(delay)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqtYwMhZeUVy"
      },
      "source": [
        "由于网络问题或OpenAI的限制会导致获取数据失败，此时脚本会跳过这部分数据\n",
        "\n",
        "重新运行下面的单元格即可补充获取失败的数据"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "gVD4JCQULHRF",
        "outputId": "80e2c81b-c964-4cb6-dc1f-c33d55892bfd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing items:  15%|█▍        | 32/218 [00:30<07:20,  2.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Error] That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 81be8ab2f02102fda5b5e433cbee8562 in your message.)\n",
            "Error saving item: /content/temp/WizardLM_tr_28.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing items:  78%|███████▊  | 170/218 [00:41<00:33,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Error] That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 0731b0918606f2e6aed561c91717f7b6 in your message.)\n",
            "Error saving item: /content/temp/WizardLM_tr_167.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing items: 100%|██████████| 218/218 [00:44<00:00,  4.94it/s]\n"
          ]
        }
      ],
      "source": [
        "await main()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 合并所有翻译数据"
      ],
      "metadata": {
        "id": "L8-Gk3KUq7Sh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "OBWIswEldVFt",
        "outputId": "2d39175d-9d2f-41d8-f7a0-d1fa759951a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 216/216 [00:00<00:00, 16289.73it/s]\n"
          ]
        }
      ],
      "source": [
        "data = []\n",
        "for filename in tqdm(os.listdir(temp_path)):\n",
        "    if filename.startswith(output_prefix) and filename.endswith(\".json\"):\n",
        "        with open(os.path.join(temp_path, filename), 'r', encoding='utf-8') as file:\n",
        "            try:\n",
        "                entry = json.load(file)\n",
        "                data.append(entry)\n",
        "            except json.JSONDecodeError:\n",
        "                pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "gV_z30iOdVFt",
        "outputId": "1ec438d3-a969-42b0-bba5-25c5a9d28137",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 216/216 [00:00<00:00, 41336.39it/s]\n"
          ]
        }
      ],
      "source": [
        "file_counter = 1\n",
        "current_file_size = 0\n",
        "output_file = f\"{output_path}/{output_prefix}_{file_counter}.jsonl\"\n",
        "\n",
        "with open(output_file, 'w', encoding='utf-8') as out:\n",
        "    for item in tqdm(data):\n",
        "        item_json = json.dumps(item, ensure_ascii=False)\n",
        "        item_size = len(item_json.encode('utf-8'))\n",
        "        out.write(item_json + \"\\n\")\n",
        "        current_file_size += item_size\n",
        "        if current_file_size > max_file_size:\n",
        "            file_counter += 1\n",
        "            output_file = f\"{output_path}/{output_prefix}_{file_counter}.jsonl\"\n",
        "            out = open(output_file, 'w', encoding='utf-8')\n",
        "            current_file_size = 0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(output_file)"
      ],
      "metadata": {
        "id": "Er0BXYxKrGTL",
        "outputId": "f5d047e6-4d12-42dd-fdca-dc72c5280c3a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/translate/WizardLM_tr_1.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2BeuFpz8rJtZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}