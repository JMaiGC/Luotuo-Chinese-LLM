{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LC1332/Luotuo-Chinese-LLM/blob/main/notebook/improvedTranslation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fB4TiOyU2aPk"
      },
      "source": [
        "# 一个升级后的批量翻译代码\n",
        "\n",
        "这个代码最初由黄泓森进行开发，由李鲁鲁转到colab并进行了更改\n",
        "\n",
        "[骆驼项目主页](https://github.com/LC1332/Luotuo-Chinese-LLM)\n",
        "\n",
        "如果你使用我们的代码获取了有用的数据，也欢迎分享给我们，或者告诉我们你公开后的github/huggingface链接\n",
        "\n",
        "如果你使用我们的代码获取数据并发表了论文或者tech report，欢迎cite我们的github repo"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 安装环境"
      ],
      "metadata": {
        "id": "YzEAdry5GRz4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "FY9B-984F_mQ"
      },
      "outputs": [],
      "source": [
        "!pip install openai\n",
        "!pip install aiofiles"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "import openai\n",
        "import asyncio\n",
        "import aiohttp\n",
        "import aiofiles\n",
        "from functools import partial\n",
        "from tqdm.asyncio import tqdm as tqdm"
      ],
      "metadata": {
        "id": "A7Ddf_CbGBCE"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 输入你的openAI API"
      ],
      "metadata": {
        "id": "v0rXCoAWGT0d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 在这里输入你的openAI API token\n",
        "\n",
        "api_key = [\"sk-r4\"]\n",
        "\n",
        "\n",
        "class KeyPool:\n",
        "    def __init__(self, strings):\n",
        "        self.pool = list(strings)\n",
        "        self.last_used = {s: -1 for s in strings}\n",
        "\n",
        "    def getKey(self):\n",
        "        result = min(self.last_used, key=self.last_used.get)\n",
        "        self.last_used[result] = int(time.time() * 1000)\n",
        "        return result\n",
        "\n",
        "pool = KeyPool(api_key)"
      ],
      "metadata": {
        "id": "IWfEZjVbGnon"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 指定工作目录\n",
        "\n"
      ],
      "metadata": {
        "id": "hGsk442ZHJcs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"/content/\")"
      ],
      "metadata": {
        "id": "e1A5kyTFHpjC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 获取需要翻译的样本\n",
        "\n",
        "这里我们使用WizardLM的样本"
      ],
      "metadata": {
        "id": "Mo43M636J3KT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/LC1332/WizardLM/main/data/WizardLM_testset.jsonl -O WizardLM_testset.jsonl"
      ],
      "metadata": {
        "id": "WTYu1i1_Jyxw",
        "outputId": "7b4d537e-7f06-4e24-bd86-c01ea2caa8bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-10 01:44:37--  https://raw.githubusercontent.com/LC1332/WizardLM/main/data/WizardLM_testset.jsonl\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 81139 (79K) [text/plain]\n",
            "Saving to: ‘WizardLM_testset.jsonl’\n",
            "\n",
            "\rWizardLM_testset.js   0%[                    ]       0  --.-KB/s               \rWizardLM_testset.js 100%[===================>]  79.24K  --.-KB/s    in 0.008s  \n",
            "\n",
            "2023-05-10 01:44:37 (9.38 MB/s) - ‘WizardLM_testset.jsonl’ saved [81139/81139]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "delay = 0.05\n",
        "\n",
        "concurrency_limit = 64\n",
        "\n",
        "input_file = \"WizardLM_testset.jsonl\"\n",
        "\n",
        "#直接输出到本来目录\n",
        "output_path = \"/content/translate\"\n",
        "\n",
        "output_prefix = f\"{output_path}/WizardLM_tr\"\n",
        "\n",
        "entries = [\"Instruction\"]\n",
        "\n",
        "!mkdir -p /content/translate"
      ],
      "metadata": {
        "id": "39UbhftwKDGH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "C7a7AdXAF_mR"
      },
      "outputs": [],
      "source": [
        "async def getTranslation(item, entries: list = []):\n",
        "    async def get(text):\n",
        "        text = text.replace(\"\\n\", \" \")\n",
        "        openai.api_key = pool.getKey()\n",
        "        try:\n",
        "            resp = await openai.ChatCompletion.acreate(\n",
        "                model=\"gpt-3.5-turbo\",\n",
        "                messages=[\n",
        "                    {\n",
        "                        \"role\": \"system\",\n",
        "                        \"content\": \"这是一个能够将文本翻译成中文的AI助手。请将引号中的文本翻译成简体中文。\",\n",
        "                    },\n",
        "                    {\"role\": \"user\", \"content\": f'\"\"\"\\n{text}\\n\"\"\"'},\n",
        "                ],\n",
        "                temperature=0.3,\n",
        "                max_tokens=100,\n",
        "                top_p=1.0,\n",
        "                frequency_penalty=0.0,\n",
        "                presence_penalty=0.0,\n",
        "            )\n",
        "            if \"choices\" in resp:\n",
        "                return resp['choices'][0]['message']['content']\n",
        "            else:\n",
        "                raise Exception(f\"Invalid API response: {resp}\")\n",
        "        except Exception as e:\n",
        "            print(f\"[Error] {e}\")\n",
        "            return None\n",
        "\n",
        "    for entry in entries:\n",
        "        trans = await get(item[entry])\n",
        "        if trans is None:\n",
        "            return None\n",
        "        else:\n",
        "            item[f\"{entry}_zh\"] = trans\n",
        "    return item\n",
        "\n",
        "\n",
        "async def process(id, item, semaphore):\n",
        "    async with semaphore:\n",
        "        file_name = f\"{output_prefix}_{id}.json\"\n",
        "        try:\n",
        "            it = await getTranslation(item, entries)\n",
        "            if it is None:\n",
        "                raise Exception(file_name)\n",
        "            async with aiofiles.open(file_name, \"w\") as f:\n",
        "                await f.write(json.dumps(it, ensure_ascii=False, indent=4))\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving item: {e}\")\n",
        "\n",
        "\n",
        "async def main():\n",
        "    try:\n",
        "      with open(input_file, \"r\") as file:\n",
        "          data = json.load(file)\n",
        "    except json.JSONDecodeError:\n",
        "      data = []\n",
        "      with open(input_file, \"r\") as file:\n",
        "          for line in file:\n",
        "              entry = json.loads(line)\n",
        "              data.append(entry)\n",
        "\n",
        "    tasks = []\n",
        "\n",
        "    semaphore = asyncio.Semaphore(concurrency_limit)\n",
        "\n",
        "    for id, item in enumerate(data):\n",
        "        if os.path.exists(f\"{output_prefix}{id}.json\"):\n",
        "            continue\n",
        "        tasks.append(asyncio.create_task(process(id, item, semaphore)))\n",
        "\n",
        "    async for task in tqdm(tasks, total=len(tasks), desc=\"Processing items\"):\n",
        "        await task\n",
        "        time.sleep(delay)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "await main()"
      ],
      "metadata": {
        "id": "gVD4JCQULHRF",
        "outputId": "c3a3cd79-186b-42e6-801d-bd296122426e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing items:  11%|█▏        | 25/218 [00:30<08:24,  2.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Error] That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID db1e211b606b633655afde30bbbf8364 in your message.)\n",
            "Error saving item: /content/translate/WizardLM_tr_21.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing items:  56%|█████▌    | 121/218 [00:37<00:29,  3.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Error] That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID c3fd6c020ae2d9b310c280305f8132a3 in your message.)\n",
            "Error saving item: /content/translate/WizardLM_tr_117.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing items: 100%|██████████| 218/218 [00:42<00:00,  5.12it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 合并所有的子文件"
      ],
      "metadata": {
        "id": "DFRRzADtV3t-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "rD3TCF7KV3bk"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_dir = output_path\n",
        "input_prefix = output_prefix\n",
        "max_file_size = 1024**3\n",
        "output_dir = \"/content/\"\n",
        "output_prefix = f\"{output_dir}wizard_translate\""
      ],
      "metadata": {
        "id": "NPyCYKd-V-vP"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = []\n",
        "for filename in tqdm(os.listdir(input_dir)):\n",
        "    if filename.startswith(input_prefix) and filename.endswith(\".json\"):\n",
        "        with open(os.path.join(input_dir, filename), 'r', encoding='utf-8') as file:\n",
        "            try:\n",
        "                entry = json.load(file)\n",
        "                data.append(entry)\n",
        "            except json.JSONDecodeError:\n",
        "                pass"
      ],
      "metadata": {
        "id": "am2vFOR7V8Nc",
        "outputId": "80e161d3-23d7-4c3f-cac8-4e4ded95e25c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 216/216 [00:00<00:00, 233196.82it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_counter = 1\n",
        "current_file_size = 0\n",
        "output_file = f\"{output_prefix}_{file_counter}.json\"\n",
        "\n",
        "with open(output_file, 'w', encoding='utf-8') as out:\n",
        "    for item in tqdm(data):\n",
        "        item_json = json.dumps(item, ensure_ascii=False)\n",
        "        item_size = len(item_json.encode('utf-8'))\n",
        "        out.write(item_json + \"\\n\")\n",
        "        current_file_size += item_size\n",
        "        if current_file_size > max_file_size:\n",
        "            file_counter += 1\n",
        "            output_file = f\"{output_prefix}{file_counter}.json\"\n",
        "            out = open(output_file, 'w', encoding='utf-8')\n",
        "            current_file_size = 0"
      ],
      "metadata": {
        "id": "oDFfrLYEV9tD",
        "outputId": "f9db1cf3-bc34-4883-a4cb-a192a9a101aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0it [00:00, ?it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mTghlBy5WeBN"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}