{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LC1332/Luotuo-Chinese-LLM/blob/main/notebook/improvedTranslation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fB4TiOyU2aPk"
      },
      "source": [
        "# 一个升级后的批量翻译代码\n",
        "\n",
        "这个代码最初由黄泓森进行开发，由李鲁鲁转到colab并进行了更改\n",
        "\n",
        "[骆驼项目主页](https://github.com/LC1332/Luotuo-Chinese-LLM)\n",
        "\n",
        "如果你使用我们的代码获取了有用的数据，也欢迎分享给我们，或者告诉我们你公开后的github/huggingface链接\n",
        "\n",
        "如果你使用我们的代码获取数据并发表了论文或者tech report，欢迎cite我们的github repo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzEAdry5GRz4"
      },
      "source": [
        "## 安装环境"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FY9B-984F_mQ",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "!pip install openai\n",
        "!pip install aiofiles\n",
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7Ddf_CbGBCE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "import openai\n",
        "import asyncio\n",
        "import aiohttp\n",
        "import aiofiles\n",
        "from functools import partial\n",
        "from tqdm.asyncio import tqdm as tqdm\n",
        "import tiktoken\n",
        "\n",
        "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
        "max_zh_en_ratio = 2.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0rXCoAWGT0d"
      },
      "source": [
        "## 输入你的openAI API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IWfEZjVbGnon"
      },
      "outputs": [],
      "source": [
        "# 在这里输入你的openAI API token\n",
        "\n",
        "api_key = [\"sk-DfFyR\"]\n",
        "\n",
        "\n",
        "class KeyPool:\n",
        "    def __init__(self, strings):\n",
        "        self.pool = list(strings)\n",
        "        self.last_used = {s: -1 for s in strings}\n",
        "\n",
        "    def getKey(self):\n",
        "        result = min(self.last_used, key=self.last_used.get)\n",
        "        self.last_used[result] = int(time.time() * 1000)\n",
        "        return result\n",
        "\n",
        "pool = KeyPool(api_key)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGsk442ZHJcs"
      },
      "source": [
        "## 指定工作目录\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1A5kyTFHpjC"
      },
      "outputs": [],
      "source": [
        "os.chdir(\"/content/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mo43M636J3KT"
      },
      "source": [
        "## 获取需要翻译的样本\n",
        "\n",
        "这里我们使用WizardLM的样本"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WTYu1i1_Jyxw",
        "outputId": "3a720f99-7de5-41da-8496-07784033877f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-12 03:22:05--  https://raw.githubusercontent.com/LC1332/WizardLM/main/data/WizardLM_testset.jsonl\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 81139 (79K) [text/plain]\n",
            "Saving to: ‘WizardLM_testset.jsonl’\n",
            "\n",
            "\rWizardLM_testset.js   0%[                    ]       0  --.-KB/s               \rWizardLM_testset.js 100%[===================>]  79.24K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2023-05-12 03:22:05 (32.6 MB/s) - ‘WizardLM_testset.jsonl’ saved [81139/81139]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/LC1332/WizardLM/main/data/WizardLM_testset.jsonl -O WizardLM_testset.jsonl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "39UbhftwKDGH",
        "outputId": "8ce1b432-a696-4b9f-e6ba-0316e46e3b2b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "delay = 0.05\n",
        "\n",
        "concurrency_limit = 32\n",
        "\n",
        "input_file = \"WizardLM_testset.jsonl\"\n",
        "\n",
        "# 数据缓存目录\n",
        "temp_path = \"/content/temp\"\n",
        "\n",
        "# 数据输出目录\n",
        "output_path = \"/content/translate\"\n",
        "\n",
        "output_prefix = \"WizardLM_tr\"\n",
        "\n",
        "max_file_size = 1024**3\n",
        "\n",
        "# 需要翻译的字段\n",
        "entries = [\"Instruction\"]\n",
        "\n",
        "os.system(f\"mkdir -p {temp_path} {output_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C7a7AdXAF_mR",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "async def getTranslation(item, entries: list = []):\n",
        "    async def get(text):\n",
        "        # text = text.replace(\"\\n\", \" \")\n",
        "        openai.api_key = pool.getKey()\n",
        "        try:\n",
        "            en_token_len = float(len(enc.encode( text )))\n",
        "            max_zh_len = int( max_zh_en_ratio * en_token_len + 10 )\n",
        "\n",
        "            messages =  [  \n",
        "            {'role':'system', 'content':'将反引号中的英文文本翻译成简体中文，并输出到一对反引号中，如`cat`->`猫`'},\n",
        "            {'role':'user', 'content':'将反引号中的指令翻译成中文:`dog`'},\n",
        "            {'role':'assistant', 'content':'`狗`'},   \n",
        "            {'role':'user', 'content':f'将反引号中的指令翻译成中文:`{text}`'}  ]\n",
        "\n",
        "            resp = await openai.ChatCompletion.acreate(\n",
        "                model=\"gpt-3.5-turbo\",\n",
        "                messages=messages,\n",
        "                temperature=0,\n",
        "                max_tokens=max_zh_len\n",
        "            )\n",
        "            if \"choices\" in resp:\n",
        "                result = resp['choices'][0]['message']['content']\n",
        "\n",
        "                result = result.strip()\n",
        "\n",
        "                if len(result) > 1 and result[0] == result[-1] == '`':  # 判断首尾字符是否是反引号\n",
        "                    return result[1:-1]  # 如果是，去掉反引号，并返回True\n",
        "                else:\n",
        "                    return result # 如果不是，返回原字符串和False\n",
        "            else:\n",
        "                raise Exception(f\"Invalid API response: {resp}\")\n",
        "        except Exception as e:\n",
        "            print(f\"[Error] {e}\")\n",
        "            return None\n",
        "\n",
        "    for entry in entries:\n",
        "        trans = await get(item[entry])\n",
        "        if trans is None:\n",
        "            return None\n",
        "        else:\n",
        "            item[f\"{entry}_zh\"] = trans\n",
        "    return item\n",
        "\n",
        "\n",
        "async def process(id, item, semaphore):\n",
        "    async with semaphore:\n",
        "        file_name = f\"{temp_path}/{output_prefix}_{id}.json\"\n",
        "        try:\n",
        "            it = await getTranslation(item, entries)\n",
        "            if it is None:\n",
        "                raise Exception(file_name)\n",
        "            async with aiofiles.open(file_name, \"w\") as f:\n",
        "                await f.write(json.dumps(it, ensure_ascii=False, indent=4))\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving item: {e}\")\n",
        "\n",
        "\n",
        "async def main():\n",
        "    try:\n",
        "      with open(input_file, \"r\") as file:\n",
        "          data = json.load(file)\n",
        "    except json.JSONDecodeError:\n",
        "      data = []\n",
        "      with open(input_file, \"r\") as file:\n",
        "          for line in file:\n",
        "              entry = json.loads(line)\n",
        "              data.append(entry)\n",
        "\n",
        "    tasks = []\n",
        "\n",
        "    semaphore = asyncio.Semaphore(concurrency_limit)\n",
        "\n",
        "    skip_count = 0\n",
        "\n",
        "    for id, item in enumerate(data):\n",
        "        if os.path.exists(f\"{temp_path}/{output_prefix}_{id}.json\"):\n",
        "            skip_count = skip_count + 1\n",
        "            continue\n",
        "        tasks.append(asyncio.create_task(process(id, item, semaphore)))\n",
        "\n",
        "    # random.shuffle( tasks )\n",
        "    print('skip ', skip_count )\n",
        "    print('rest ', len(tasks))\n",
        "\n",
        "    async for task in tqdm(tasks, total=len(tasks), desc=\"Processing items\"):\n",
        "        await task\n",
        "        time.sleep(delay)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqtYwMhZeUVy"
      },
      "source": [
        "由于网络问题或OpenAI的限制会导致获取数据失败，此时脚本会跳过这部分数据\n",
        "\n",
        "重新运行下面的单元格即可补充获取失败的数据"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVD4JCQULHRF",
        "outputId": "229ca8b1-c240-44e1-d013-9ba3883d8cf1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing items:  42%|████▏     | 91/218 [00:59<01:58,  1.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Error] That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID cc8490f290fe5d719cfce5ae3f78a5d2 in your message.)\n",
            "Error saving item: /content/temp/WizardLM_tr_172.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing items: 100%|██████████| 218/218 [01:21<00:00,  2.68it/s]\n"
          ]
        }
      ],
      "source": [
        "await main()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 合并所有翻译数据"
      ],
      "metadata": {
        "id": "L8-Gk3KUq7Sh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBWIswEldVFt",
        "outputId": "99f68892-c381-4de6-b084-13bbd217e933",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 217/217 [00:00<00:00, 15491.89it/s]\n"
          ]
        }
      ],
      "source": [
        "data = []\n",
        "for filename in tqdm(os.listdir(temp_path)):\n",
        "    if filename.startswith(output_prefix) and filename.endswith(\".json\"):\n",
        "        with open(os.path.join(temp_path, filename), 'r', encoding='utf-8') as file:\n",
        "            try:\n",
        "                entry = json.load(file)\n",
        "                data.append(entry)\n",
        "            except json.JSONDecodeError:\n",
        "                pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gV_z30iOdVFt",
        "outputId": "6a3b5ded-43b1-4725-99d8-918ef29da3ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 217/217 [00:00<00:00, 50116.40it/s]\n"
          ]
        }
      ],
      "source": [
        "file_counter = 1\n",
        "current_file_size = 0\n",
        "output_file = f\"{output_path}/{output_prefix}_{file_counter}.jsonl\"\n",
        "\n",
        "with open(output_file, 'w', encoding='utf-8') as out:\n",
        "    for item in tqdm(data):\n",
        "        item_json = json.dumps(item, ensure_ascii=False)\n",
        "        item_size = len(item_json.encode('utf-8'))\n",
        "        out.write(item_json + \"\\n\")\n",
        "        current_file_size += item_size\n",
        "        if current_file_size > max_file_size:\n",
        "            file_counter += 1\n",
        "            output_file = f\"{output_path}/{output_prefix}_{file_counter}.jsonl\"\n",
        "            out = open(output_file, 'w', encoding='utf-8')\n",
        "            current_file_size = 0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(output_file)"
      ],
      "metadata": {
        "id": "Er0BXYxKrGTL",
        "outputId": "65cfb3ca-3c08-4bfb-8d39-90df816dcffd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/translate/WizardLM_tr_1.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2BeuFpz8rJtZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}