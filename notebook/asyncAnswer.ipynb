{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LC1332/Luotuo-Chinese-LLM/blob/main/notebook/asyncAnswer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fB4TiOyU2aPk"
      },
      "source": [
        "# 批量回答的代码\n",
        "\n",
        "代码最初由李鲁鲁开发\n",
        "\n",
        "[骆驼项目主页](https://github.com/LC1332/Luotuo-Chinese-LLM)\n",
        "\n",
        "如果你使用我们的代码获取了有用的数据，也欢迎分享给我们，或者告诉我们你公开后的github/huggingface链接\n",
        "\n",
        "如果你使用我们的代码获取数据并发表了论文或者tech report，欢迎cite我们的github repo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzEAdry5GRz4"
      },
      "source": [
        "## 安装环境"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FY9B-984F_mQ",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "!pip install openai\n",
        "!pip install aiofiles\n",
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "A7Ddf_CbGBCE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "import openai\n",
        "import asyncio\n",
        "import aiohttp\n",
        "import aiofiles\n",
        "from functools import partial\n",
        "from tqdm.asyncio import tqdm as tqdm\n",
        "import tiktoken\n",
        "\n",
        "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
        "max_zh_en_ratio = 2.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0rXCoAWGT0d"
      },
      "source": [
        "## 输入你的openAI API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "IWfEZjVbGnon"
      },
      "outputs": [],
      "source": [
        "# 在这里输入你的openAI API token\n",
        "\n",
        "api_key = [\"sk-DfFy\"]\n",
        "\n",
        "\n",
        "class KeyPool:\n",
        "    def __init__(self, strings):\n",
        "        self.pool = list(strings)\n",
        "        self.last_used = {s: -1 for s in strings}\n",
        "\n",
        "    def getKey(self):\n",
        "        result = min(self.last_used, key=self.last_used.get)\n",
        "        self.last_used[result] = int(time.time() * 1000)\n",
        "        return result\n",
        "\n",
        "pool = KeyPool(api_key)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGsk442ZHJcs"
      },
      "source": [
        "## 指定工作目录\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "e1A5kyTFHpjC"
      },
      "outputs": [],
      "source": [
        "os.chdir(\"/content/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mo43M636J3KT"
      },
      "source": [
        "## 获取需要翻译的样本\n",
        "\n",
        "这里我们使用WizardLM的样本"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "WTYu1i1_Jyxw",
        "outputId": "0b12718c-613b-4108-dd6a-c30da3807e13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-12 23:56:35--  https://raw.githubusercontent.com/LC1332/WizardLM/main/data/WizardLM_train10k.jsonl\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 24924875 (24M) [text/plain]\n",
            "Saving to: ‘WizardLM_train10k.jsonl’\n",
            "\n",
            "WizardLM_train10k.j 100%[===================>]  23.77M   158MB/s    in 0.2s    \n",
            "\n",
            "2023-05-12 23:56:36 (158 MB/s) - ‘WizardLM_train10k.jsonl’ saved [24924875/24924875]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/LC1332/WizardLM/main/data/WizardLM_train10k.jsonl -O WizardLM_train10k.jsonl"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "给定一个jsonl文件 WizardLM_train10k.jsonl ，每行是一个json，注意文件中有中文，读入这个文件的前20行并保存到Wizard_demo.jsonl"
      ],
      "metadata": {
        "id": "BDoBJouuZUfH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# 读取前20行并保存到文件\n",
        "with open('WizardLM_train10k.jsonl','r', encoding='utf-8') as f:\n",
        "    data = [json.loads(next(f)) for x in range(20)]\n",
        "    with open('Wizard_demo.jsonl', 'w', encoding='utf-8') as f_out:\n",
        "        for d in data:\n",
        "            f_out.write(json.dumps(d, ensure_ascii=False) + '\\n')"
      ],
      "metadata": {
        "id": "FXEBPOG-ZTrc"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "39UbhftwKDGH",
        "outputId": "e3d3a802-5b34-4549-8faa-0c19ce5d78c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "delay = 0.05 * 2\n",
        "\n",
        "concurrency_limit = 16\n",
        "\n",
        "input_file = \"Wizard_demo.jsonl\"\n",
        "\n",
        "# 数据缓存目录\n",
        "temp_path = \"/content/tempAns\"\n",
        "\n",
        "# 数据输出目录\n",
        "output_path = \"/content/answer\"\n",
        "\n",
        "output_prefix = \"WizardLM_Ans\"\n",
        "\n",
        "max_file_size = 1024**3\n",
        "\n",
        "# 需要提问的字段\n",
        "entries = [\"instruction_zh\"]\n",
        "\n",
        "# 需要保存到的字段\n",
        "save_entries = [\"output_zh\"]\n",
        "\n",
        "# 需要参考长度的字段\n",
        "ref_entries = [\"output\"]\n",
        "\n",
        "\n",
        "os.system(f\"mkdir -p {temp_path} {output_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "C7a7AdXAF_mR",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "async def getTranslation(item, entries: list = []):\n",
        "    async def get(text, ans_en ):\n",
        "        # text = text.replace(\"\\n\", \" \")\n",
        "        openai.api_key = pool.getKey()\n",
        "        try:\n",
        "            if ans_en == '':\n",
        "              max_zh_len = 150\n",
        "            else:\n",
        "              en_token_len = float(len(enc.encode( ans_en )))\n",
        "              max_zh_len = int( max_zh_en_ratio * en_token_len )\n",
        "              max_zh_len = max(10, max_zh_len)\n",
        "\n",
        "            ans_len = float(len(enc.encode( text )))\n",
        "\n",
        "            messages =  [   {'role':'user', 'content':text}  ]\n",
        "\n",
        "            if max_zh_len + ans_len + 100 > 4096:\n",
        "              max_zh_len = 4096 - 100 - ans_len\n",
        "              print('shorten answer len into ', max_zh_len, ' with ans len = ', ans_len )\n",
        "\n",
        "            resp = await openai.ChatCompletion.acreate(\n",
        "                model=\"gpt-3.5-turbo\",\n",
        "                messages=messages,\n",
        "                temperature=0,\n",
        "                max_tokens=max_zh_len\n",
        "            )\n",
        "            if \"choices\" in resp:\n",
        "                result = resp['choices'][0]['message']['content']\n",
        "\n",
        "                result = result.strip()\n",
        "\n",
        "                return result\n",
        "            else:\n",
        "                raise Exception(f\"Invalid API response: {resp}\")\n",
        "        except Exception as e:\n",
        "            print(f\"[Error] {e}\")\n",
        "            return None\n",
        "\n",
        "    for i in range(len(entries)):\n",
        "        entry = entries[i]\n",
        "        save_entry = save_entries[i]\n",
        "        ref_entry = ref_entries[i]\n",
        "\n",
        "        ans = await get(item[entry], item[ref_entry])\n",
        "        if ans is None:\n",
        "            return None\n",
        "        else:\n",
        "            item[save_entry] = ans\n",
        "    return item\n",
        "\n",
        "\n",
        "async def process(id, item, semaphore):\n",
        "    async with semaphore:\n",
        "        file_name = f\"{temp_path}/{output_prefix}_{id}.json\"\n",
        "        try:\n",
        "            print('start ', id )\n",
        "            it = await getTranslation(item, entries)\n",
        "            if it is None:\n",
        "                raise Exception(file_name)\n",
        "            async with aiofiles.open(file_name, \"w\") as f:\n",
        "                await f.write(json.dumps(it, ensure_ascii=False, indent=4))\n",
        "            print('done ', id )\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving item: {e}\")\n",
        "\n",
        "\n",
        "async def main():\n",
        "    try:\n",
        "      with open(input_file, \"r\") as file:\n",
        "          data = json.load(file)\n",
        "    except json.JSONDecodeError:\n",
        "      data = []\n",
        "      with open(input_file, \"r\") as file:\n",
        "          for line in file:\n",
        "              entry = json.loads(line)\n",
        "              data.append(entry)\n",
        "\n",
        "    tasks = []\n",
        "\n",
        "    semaphore = asyncio.Semaphore(concurrency_limit)\n",
        "\n",
        "    skip_count = 0\n",
        "\n",
        "    for id, item in enumerate(data):\n",
        "        file_name = f\"{temp_path}/{output_prefix}_{id}.json\"\n",
        "        if os.path.exists(file_name):\n",
        "            skip_count = skip_count + 1\n",
        "            continue\n",
        "        tasks.append(asyncio.create_task(process(id, item, semaphore)))\n",
        "\n",
        "    print('skip ', skip_count )\n",
        "    print('rest ', len(tasks))\n",
        "\n",
        "    async for task in tqdm(tasks, total=len(tasks), desc=\"Processing items\"):\n",
        "        await task\n",
        "        time.sleep(delay)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqtYwMhZeUVy"
      },
      "source": [
        "由于网络问题或OpenAI的限制会导致获取数据失败，此时脚本会跳过这部分数据\n",
        "\n",
        "重新运行下面的单元格即可补充获取失败的数据"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "gVD4JCQULHRF",
        "outputId": "0c8c57e2-89af-4c4f-e34f-3f36fa71f727",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing items:   0%|          | 0/20 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start  0\n",
            "start  1\n",
            "start  2\n",
            "start  3\n",
            "start  4\n",
            "start  5\n",
            "start  6\n",
            "start  7\n",
            "start  8\n",
            "start  9\n",
            "start  10\n",
            "start  11\n",
            "start  12\n",
            "start  13\n",
            "start  14\n",
            "start  15\n",
            "done  12\n",
            "start  16\n",
            "done  11\n",
            "start  17\n",
            "done  13\n",
            "start  18\n",
            "done  2\n",
            "start  19\n",
            "done  14\n",
            "done  17\n",
            "done  6\n",
            "done  3\n",
            "done  19\n",
            "done  8\n",
            "done  9\n",
            "done  1\n",
            "done  10\n",
            "done  16\n",
            "done  18\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing items:  10%|█         | 2/20 [00:43<06:30, 21.69s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done  0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing items:  30%|███       | 6/20 [00:50<01:21,  5.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done  4\n",
            "done  15\n",
            "done  7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing items:  35%|███▌      | 7/20 [00:57<01:19,  6.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done  5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing items: 100%|██████████| 20/20 [00:58<00:00,  2.94s/it]\n"
          ]
        }
      ],
      "source": [
        "await main()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 合并所有翻译数据"
      ],
      "metadata": {
        "id": "L8-Gk3KUq7Sh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "OBWIswEldVFt",
        "outputId": "8bf03da2-f528-4ad8-c16e-0a2ecececd5d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20/20 [00:00<00:00, 4261.85it/s]\n"
          ]
        }
      ],
      "source": [
        "data = []\n",
        "for filename in tqdm(os.listdir(temp_path)):\n",
        "    if filename.startswith(output_prefix) and filename.endswith(\".json\"):\n",
        "        with open(os.path.join(temp_path, filename), 'r', encoding='utf-8') as file:\n",
        "            try:\n",
        "                entry = json.load(file)\n",
        "                data.append(entry)\n",
        "            except json.JSONDecodeError:\n",
        "                pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "gV_z30iOdVFt",
        "outputId": "3f521b4b-1092-4ba9-c0ec-7459cb6f10d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20/20 [00:00<00:00, 11826.60it/s]\n"
          ]
        }
      ],
      "source": [
        "file_counter = 1\n",
        "current_file_size = 0\n",
        "output_file = f\"{output_path}/{output_prefix}_{file_counter}.jsonl\"\n",
        "\n",
        "with open(output_file, 'w', encoding='utf-8') as out:\n",
        "    for item in tqdm(data):\n",
        "        item_json = json.dumps(item, ensure_ascii=False)\n",
        "        item_size = len(item_json.encode('utf-8'))\n",
        "        out.write(item_json + \"\\n\")\n",
        "        current_file_size += item_size\n",
        "        if current_file_size > max_file_size:\n",
        "            file_counter += 1\n",
        "            output_file = f\"{output_path}/{output_prefix}_{file_counter}.jsonl\"\n",
        "            out = open(output_file, 'w', encoding='utf-8')\n",
        "            current_file_size = 0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(output_file)"
      ],
      "metadata": {
        "id": "Er0BXYxKrGTL",
        "outputId": "6a5632f0-753d-4812-e216-6baa6dd8a97d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/answer/WizardLM_Ans_1.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2BeuFpz8rJtZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}