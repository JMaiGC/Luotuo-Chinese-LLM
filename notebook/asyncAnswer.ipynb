{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LC1332/Luotuo-Chinese-LLM/blob/main/notebook/improvedTranslation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fB4TiOyU2aPk"
      },
      "source": [
        "# 一个升级后的批量翻译代码\n",
        "\n",
        "这个代码最初由黄泓森进行开发，由李鲁鲁转到colab并进行了更改\n",
        "\n",
        "[骆驼项目主页](https://github.com/LC1332/Luotuo-Chinese-LLM)\n",
        "\n",
        "如果你使用我们的代码获取了有用的数据，也欢迎分享给我们，或者告诉我们你公开后的github/huggingface链接\n",
        "\n",
        "如果你使用我们的代码获取数据并发表了论文或者tech report，欢迎cite我们的github repo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzEAdry5GRz4"
      },
      "source": [
        "## 安装环境"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FY9B-984F_mQ",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "!pip install openai\n",
        "!pip install aiofiles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "A7Ddf_CbGBCE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "import openai\n",
        "import asyncio\n",
        "import aiohttp\n",
        "import aiofiles\n",
        "from functools import partial\n",
        "from tqdm.asyncio import tqdm as tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0rXCoAWGT0d"
      },
      "source": [
        "## 输入你的openAI API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "IWfEZjVbGnon"
      },
      "outputs": [],
      "source": [
        "# 在这里输入你的openAI API token\n",
        "\n",
        "api_key = [\"sk-lGa\"]\n",
        "\n",
        "\n",
        "class KeyPool:\n",
        "    def __init__(self, strings):\n",
        "        self.pool = list(strings)\n",
        "        self.last_used = {s: -1 for s in strings}\n",
        "\n",
        "    def getKey(self):\n",
        "        result = min(self.last_used, key=self.last_used.get)\n",
        "        self.last_used[result] = int(time.time() * 1000)\n",
        "        return result\n",
        "\n",
        "pool = KeyPool(api_key)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGsk442ZHJcs"
      },
      "source": [
        "## 指定工作目录\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "e1A5kyTFHpjC"
      },
      "outputs": [],
      "source": [
        "os.chdir(\"/content/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mo43M636J3KT"
      },
      "source": [
        "## 获取需要翻译的样本\n",
        "\n",
        "这里我们使用WizardLM的样本"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "WTYu1i1_Jyxw",
        "outputId": "03457baa-ec89-47ab-b1bc-1a439a4274a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-10 05:47:11--  https://raw.githubusercontent.com/LC1332/WizardLM/main/data/WizardLM_testset.jsonl\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 81139 (79K) [text/plain]\n",
            "Saving to: ‘WizardLM_testset.jsonl’\n",
            "\n",
            "WizardLM_testset.js 100%[===================>]  79.24K  --.-KB/s    in 0.004s  \n",
            "\n",
            "2023-05-10 05:47:11 (18.6 MB/s) - ‘WizardLM_testset.jsonl’ saved [81139/81139]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/LC1332/WizardLM/main/data/WizardLM_testset.jsonl -O WizardLM_testset.jsonl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "39UbhftwKDGH",
        "outputId": "e20439a4-8d92-4066-b97f-4e1e8f6e97d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ],
      "source": [
        "delay = 0.05\n",
        "\n",
        "concurrency_limit = 32\n",
        "\n",
        "input_file = \"WizardLM_testset.jsonl\"\n",
        "\n",
        "# 数据缓存目录\n",
        "temp_path = \"/content/temp\"\n",
        "\n",
        "# 数据输出目录\n",
        "output_path = \"/content/translate\"\n",
        "\n",
        "output_prefix = \"WizardLM_tr\"\n",
        "\n",
        "max_file_size = 1024**3\n",
        "\n",
        "# 需要翻译的字段\n",
        "entries = [\"Instruction\"]\n",
        "\n",
        "os.system(f\"mkdir -p {temp_path} {output_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "C7a7AdXAF_mR",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "async def getTranslation(item, entries: list = []):\n",
        "    async def get(text):\n",
        "        text = text.replace(\"\\n\", \" \")\n",
        "        openai.api_key = pool.getKey()\n",
        "        try:\n",
        "            messages =  [  \n",
        "            {'role':'system', 'content':'你是一个能够将文本翻译成中文的AI助手。请将反引号中的英文文本翻译成简体中文。'},    \n",
        "            {'role':'user', 'content':'将反引号中的指令翻译成中文:```ResNet mainly utilizes residual blocks like f(x)+x, which makes the backpropagation smoother```'},   \n",
        "            {'role':'assistant', 'content':'```ResNet主要利用了形如f(x)+x的残差Block，使得反向传播可以更加顺利。```'},   \n",
        "            {'role':'user', 'content':'将反引号中的指令翻译成中文:```Who are you?```'},\n",
        "            {'role':'assistant', 'content':'```你是谁?```'},\n",
        "            {'role':'user', 'content':f'将反引号中的指令翻译成中文:```{text}```'}  ]\n",
        "\n",
        "            resp = await openai.ChatCompletion.acreate(\n",
        "                model=\"gpt-3.5-turbo\",\n",
        "                messages=messages,\n",
        "                temperature=0,\n",
        "                max_tokens=500,\n",
        "                top_p=1.0,\n",
        "                frequency_penalty=0.0,\n",
        "                presence_penalty=0.0,\n",
        "            )\n",
        "            if \"choices\" in resp:\n",
        "                my_str = resp['choices'][0]['message']['content']\n",
        "\n",
        "                pattern = r'```([^\\n]+)```'\n",
        "                # 搜索匹配的结果\n",
        "                match_res = re.search(pattern, my_str)\n",
        "                if match_res:\n",
        "                  first_result = match_res.group(1)\n",
        "                  # 输出第一组反引号中间的内容\n",
        "                  if match_res and (len(my_str) - len(first_result) == 6 or len(my_str) - len(first_result) == 7):\n",
        "                      return first_result\n",
        "                  else:\n",
        "                      return my_str\n",
        "                else:\n",
        "                  return my_str\n",
        "            else:\n",
        "                raise Exception(f\"Invalid API response: {resp}\")\n",
        "        except Exception as e:\n",
        "            print(f\"[Error] {e}\")\n",
        "            return None\n",
        "\n",
        "    for entry in entries:\n",
        "        trans = await get(item[entry])\n",
        "        if trans is None:\n",
        "            return None\n",
        "        else:\n",
        "            item[f\"{entry}_zh\"] = trans\n",
        "    return item\n",
        "\n",
        "\n",
        "async def process(id, item, semaphore):\n",
        "    async with semaphore:\n",
        "        file_name = f\"{temp_path}/{output_prefix}_{id}.json\"\n",
        "        try:\n",
        "            it = await getTranslation(item, entries)\n",
        "            if it is None:\n",
        "                raise Exception(file_name)\n",
        "            async with aiofiles.open(file_name, \"w\") as f:\n",
        "                await f.write(json.dumps(it, ensure_ascii=False, indent=4))\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving item: {e}\")\n",
        "\n",
        "\n",
        "async def main():\n",
        "    try:\n",
        "      with open(input_file, \"r\") as file:\n",
        "          data = json.load(file)\n",
        "    except json.JSONDecodeError:\n",
        "      data = []\n",
        "      with open(input_file, \"r\") as file:\n",
        "          for line in file:\n",
        "              entry = json.loads(line)\n",
        "              data.append(entry)\n",
        "\n",
        "    tasks = []\n",
        "\n",
        "    semaphore = asyncio.Semaphore(concurrency_limit)\n",
        "\n",
        "    for id, item in enumerate(data):\n",
        "        if os.path.exists(f\"{output_prefix}{id}.json\"):\n",
        "            continue\n",
        "        tasks.append(asyncio.create_task(process(id, item, semaphore)))\n",
        "\n",
        "    async for task in tqdm(tasks, total=len(tasks), desc=\"Processing items\"):\n",
        "        await task\n",
        "        time.sleep(delay)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqtYwMhZeUVy"
      },
      "source": [
        "由于网络问题或OpenAI的限制会导致获取数据失败，此时脚本会跳过这部分数据\n",
        "\n",
        "重新运行下面的单元格即可补充获取失败的数据"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "gVD4JCQULHRF",
        "outputId": "dcc018a9-6c0b-4c20-8b07-1907377b6437",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing items:  22%|██▏       | 48/218 [00:33<06:24,  2.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Error] That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 33e63188de534230abe6ce3fdd2fc5f3 in your message.)\n",
            "Error saving item: /content/temp/WizardLM_tr_45.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing items:  98%|█████████▊| 214/218 [01:16<00:08,  2.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Error] That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID ee5494559f0f454f82e7c42507c7fc23 in your message.)\n",
            "Error saving item: /content/temp/WizardLM_tr_211.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing items: 100%|██████████| 218/218 [01:16<00:00,  2.84it/s]\n"
          ]
        }
      ],
      "source": [
        "await main()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 合并所有翻译数据"
      ],
      "metadata": {
        "id": "L8-Gk3KUq7Sh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "OBWIswEldVFt",
        "outputId": "fef86723-7ebc-407d-b324-88f963cd9e2c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 217/217 [00:00<00:00, 14409.31it/s]\n"
          ]
        }
      ],
      "source": [
        "data = []\n",
        "for filename in tqdm(os.listdir(temp_path)):\n",
        "    if filename.startswith(output_prefix) and filename.endswith(\".json\"):\n",
        "        with open(os.path.join(temp_path, filename), 'r', encoding='utf-8') as file:\n",
        "            try:\n",
        "                entry = json.load(file)\n",
        "                data.append(entry)\n",
        "            except json.JSONDecodeError:\n",
        "                pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "gV_z30iOdVFt",
        "outputId": "268f1b6b-27ed-4391-c3cf-0755f9d7311a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 217/217 [00:00<00:00, 46210.60it/s]\n"
          ]
        }
      ],
      "source": [
        "file_counter = 1\n",
        "current_file_size = 0\n",
        "output_file = f\"{output_path}/{output_prefix}_{file_counter}.jsonl\"\n",
        "\n",
        "with open(output_file, 'w', encoding='utf-8') as out:\n",
        "    for item in tqdm(data):\n",
        "        item_json = json.dumps(item, ensure_ascii=False)\n",
        "        item_size = len(item_json.encode('utf-8'))\n",
        "        out.write(item_json + \"\\n\")\n",
        "        current_file_size += item_size\n",
        "        if current_file_size > max_file_size:\n",
        "            file_counter += 1\n",
        "            output_file = f\"{output_path}/{output_prefix}_{file_counter}.jsonl\"\n",
        "            out = open(output_file, 'w', encoding='utf-8')\n",
        "            current_file_size = 0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(output_file)"
      ],
      "metadata": {
        "id": "Er0BXYxKrGTL",
        "outputId": "6735cd46-19a5-4841-aec5-d206cbb3800e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/translate/WizardLM_tr_1.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2BeuFpz8rJtZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}